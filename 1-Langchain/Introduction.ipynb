{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from from_root import from_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(from_root(\".env\"), override=True)           #Restart the kernel if same environment variable is getting displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Langchain Tracking & Tracing\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "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\n",
      "OPENAI-Project\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "print(os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "print(os.getenv(\"LANGCHAIN_TRACING_V2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000002A89B8F2C00> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002A891F9FCE0> root_client=<openai.OpenAI object at 0x000002A891F9FDD0> root_async_client=<openai.AsyncOpenAI object at 0x000002A89B9370B0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(\"what is Agentic AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems that exhibit agency, which means they have the capacity to make decisions, take actions, and potentially learn or adapt based on their interactions with the environment. These AI systems are designed to operate autonomously to achieve specific goals or tasks without constant human guidance or intervention. \n",
      "\n",
      "The concept of agentic AI often involves the incorporation of advanced algorithms that enable decision-making processes, situational awareness, and sometimes interactive learning capabilities. These systems might range from simple automated processes to more complex models like autonomous vehicles, robotic systems, and certain software agents that perform tasks like trading, resource management, or personal assistance.\n",
      "\n",
      "The development and implementation of agentic AI also come with considerations around ethical use, control, transparency, and accountability, as these systems can have significant impacts on their environments and human interactions.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a set of tools and services designed to enhance the capabilities of language-based applications and workflows. Built around LangChain, a popular framework for developing applications with large language models (LLMs), Langsmith offers features that provide observability, testing, and monitoring. This enables developers to better understand and optimize their applications by gaining insights into how they perform in various scenarios. It also facilitates experimentation, ensuring that applications remain reliable and effective as they evolve or as underlying language models are updated.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 31, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None} id='run-07327b6f-c8f6-48f2-b39a-dd2fa964761d-0' usage_metadata={'input_tokens': 31, 'output_tokens': 99, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a set of tools and services designed to enhance the capabilities of language-based applications and workflows. Built around LangChain, a popular framework for developing applications with large language models (LLMs), Langsmith offers features that provide observability, testing, and monitoring. This enables developers to better understand and optimize their applications by gaining insights into how they perform in various scenarios. It also facilitates experimentation, ensuring that applications remain reliable and effective as they evolve or as underlying language models are updated.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The attention mechanism is a concept in neural networks that allows models to dynamically focus on different parts of the input sequence when producing each part of the output sequence. It's a crucial element in many state-of-the-art models, especially in natural language processing tasks, and has been particularly influential in the development of transformers.\n",
      "\n",
      "Here's how the attention mechanism works, especially in the context of sequence-to-sequence models:\n",
      "\n",
      "1. **Contextual Focus**: Instead of processing an entire input sequence with equal emphasis at each step, the attention mechanism allows the model to consider a weighted combination of all input positions for generating an output element. This mimics the human ability to focus selectively on certain parts of a sentence while understanding or translating language.\n",
      "\n",
      "2. **Attention Scores**: For each output element, attention scores are computed, which quantify the relevance or importance of each input element to the current output element being generated. This is typically done using a compatibility function (such as dot product or a learned feed-forward network) to compare the current hidden state of the decoder with each hidden state of the encoder.\n",
      "\n",
      "3. **Softmax Normalization**: The attention scores are normalized using a softmax function, converting them into a probability distribution. This ensures the scores sum to one but also accentuates the difference between high and low scores.\n",
      "\n",
      "4. **Weighted Sum**: These normalized attention scores are then used to compute a weighted sum of the encoder hidden states. This produces a context vector that captures the relevant information from the input sequence for generating the current output element.\n",
      "\n",
      "5. **Integration and Output**: The context vector is combined with the current state of the decoder to produce the next output element. This can be achieved through concatenation and passing through a dense layer or more complex integration strategies.\n",
      "\n",
      "The attention mechanism enhances the model's ability to handle long-range dependencies and capture intricate patterns across the input sequence. It helps in issues like vanishing gradients that hinder previous sequential models such as RNNs. In particular, the self-attention mechanism, a variant used in transformers, allows models to relate different positions of a single sequence to efficiently handle dependencies, making transformers highly effective for various tasks.\n"
     ]
    }
   ],
   "source": [
    "# Formatting the Output\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you explain attention mechanism?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DeepLearning': {'Definition': \"Deep Learning is a subset of machine learning that uses neural networks with many layers (hence 'deep') to analyze various levels of data hierarchies.\", 'Architecture': {'NeuralNetwork': {'Layers': ['Input Layer', 'Hidden Layers', 'Output Layer'], 'Nodes': 'Also known as neurons, they are the fundamental units where data processing occurs.', 'ActivationFunctions': 'Includes functions like ReLU, Sigmoid, and Tanh that introduce non-linearity to the model.'}}, 'Training': {'Backpropagation': 'An algorithm used to minimize the loss by adjusting the weights in the network.', 'GradientDescent': 'Used to optimize the weights to find the minimum of loss functions.'}, 'DataRequirements': 'Deep learning models often require large amounts of data and substantial computational power.', 'Applications': ['ImageRecognition', 'Natural Language Processing', 'Speech Recognition', 'Autonomous Vehicles'], 'Advantages': {'Accuracy': 'Can achieve high accuracy with enough data and tuning.', 'FeatureExtraction': 'Automatically extracts features without human intervention.'}, 'Challenges': {'Overfitting': 'Risk of the model performing well on training data but poorly on new data.', 'Computation': 'Requires significant resources and processing power.', 'DataDependency': 'Performance heavily relies on the quantity and quality of data available.'}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "json_output_parser=JsonOutputParser()\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"Answer the user query: \\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": json_output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain=prompt|llm|json_output_parser\n",
    "response=chain.invoke({\"query\":\"Can you explain Deep Learning?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
