{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000270E000A450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000270E001BDA0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã\\n\\nHow can I help you today? üòä\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 11, 'total_tokens': 26, 'completion_time': 0.027272727, 'prompt_time': 0.001922015, 'queue_time': 0.017902073, 'total_time': 0.029194742}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eaf1e74-3565-466b-b95b-9a94a9a37bfe-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bala! It's nice to meet you. \\n\\nWhat kind of data science work do you do?  I'm always interested in learning more about how people are using AI and data to solve problems.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 21, 'total_tokens': 68, 'completion_time': 0.085454545, 'prompt_time': 0.002176465, 'queue_time': 0.021424804000000002, 'total_time': 0.08763101}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2d65cd11-efdf-4cf5-a846-95932fcb0d7d-0', usage_metadata={'input_tokens': 21, 'output_tokens': 47, 'total_tokens': 68})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Bala and I am a Data Scientist\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, your name is Bala. üòä \\n\\nHow can I help you today?  Are you working on any interesting projects?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 47, 'total_tokens': 76, 'completion_time': 0.052727273, 'prompt_time': 0.003374671, 'queue_time': 0.017248577, 'total_time': 0.056101944}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-eb1e7019-c450-4ecb-9e12-007cdb91bee5-0', usage_metadata={'input_tokens': 47, 'output_tokens': 29, 'total_tokens': 76})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Bala and I am a Data Scientist\"),\n",
    "              AIMessage(content=\"Hi Bala, How are you ?\"),\n",
    "              HumanMessage(content=\"Am fine, Can you tell my name ?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above method is not the right option to store the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "\"\"\"\n",
    "ChatMessageHistory -> is a single chat history\n",
    "BaseChatMessageHistory -> is a ccollection of chat history\n",
    "\"\"\"\n",
    "\n",
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x00000270E0E47600>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bala! It's nice to meet you. \\n\\nWhat kind of data science work do you do?  I'm always interested in learning more about how people use AI and data analysis in their fields.  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 20, 'total_tokens': 69, 'completion_time': 0.089090909, 'prompt_time': 0.001926475, 'queue_time': 0.018873174, 'total_time': 0.091017384}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-43d9ba27-1541-4814-8fc4-ea8ecbc14df4-0', usage_metadata={'input_tokens': 20, 'output_tokens': 49, 'total_tokens': 69})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"Bala\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, My name is Bala and I a Data Scientist\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bala.  I remember that from our introduction! üòä  How can I help you today? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 80, 'total_tokens': 106, 'completion_time': 0.047272727, 'prompt_time': 0.004027649, 'queue_time': 0.020362719999999997, 'total_time': 0.051300376}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-215e5002-0a74-40dc-bc3c-d0071b5454a0-0', usage_metadata={'input_tokens': 80, 'output_tokens': 26, 'total_tokens': 106})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 13, 'total_tokens': 56, 'completion_time': 0.078181818, 'prompt_time': 0.001897096, 'queue_time': 0.019587551999999998, 'total_time': 0.080078914}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-009ac799-2b5b-4fb2-a47d-7526e6321f92-0', usage_metadata={'input_tokens': 13, 'output_tokens': 43, 'total_tokens': 56})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"Ramya\"}}\n",
    "\n",
    "response2=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config\n",
    ")\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As a large language model, I have no memory of past conversations. Every interaction we have is a fresh start.\\n\\nWhat can I help you with today? üòä  \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 71, 'total_tokens': 109, 'completion_time': 0.069090909, 'prompt_time': 0.00404956, 'queue_time': 0.019557789, 'total_time': 0.073140469}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e6d5d977-9074-4cb4-8c70-88ec432315bd-0', usage_metadata={'input_tokens': 71, 'output_tokens': 38, 'total_tokens': 109})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What did I ask you earlier?\")],\n",
    "    config=config\n",
    ")\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Bala and I a Data Scientist', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Bala! It's nice to meet you. \\n\\nWhat kind of data science work do you do?  I'm always interested in learning more about how people use AI and data analysis in their fields.  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 20, 'total_tokens': 69, 'completion_time': 0.089090909, 'prompt_time': 0.001926475, 'queue_time': 0.018873174, 'total_time': 0.091017384}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-43d9ba27-1541-4814-8fc4-ea8ecbc14df4-0', usage_metadata={'input_tokens': 20, 'output_tokens': 49, 'total_tokens': 69}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Bala.  I remember that from our introduction! üòä  How can I help you today? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 80, 'total_tokens': 106, 'completion_time': 0.047272727, 'prompt_time': 0.004027649, 'queue_time': 0.020362719999999997, 'total_time': 0.051300376}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-215e5002-0a74-40dc-bc3c-d0071b5454a0-0', usage_metadata={'input_tokens': 80, 'output_tokens': 26, 'total_tokens': 106})])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"Bala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As a large language model, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 13, 'total_tokens': 56, 'completion_time': 0.078181818, 'prompt_time': 0.001897096, 'queue_time': 0.019587551999999998, 'total_time': 0.080078914}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-009ac799-2b5b-4fb2-a47d-7526e6321f92-0', usage_metadata={'input_tokens': 13, 'output_tokens': 43, 'total_tokens': 56}), HumanMessage(content='What did I ask you earlier?', additional_kwargs={}, response_metadata={}), AIMessage(content='As a large language model, I have no memory of past conversations. Every interaction we have is a fresh start.\\n\\nWhat can I help you with today? üòä  \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 71, 'total_tokens': 109, 'completion_time': 0.069090909, 'prompt_time': 0.00404956, 'queue_time': 0.019557789, 'total_time': 0.073140469}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e6d5d977-9074-4cb4-8c70-88ec432315bd-0', usage_metadata={'input_tokens': 71, 'output_tokens': 38, 'total_tokens': 109})])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"Ramya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's awesome, Jhon Cena! \\n\\nVanilla ice cream is a classic for a reason. It's simple, delicious, and goes well with just about anything. \\n\\nDo you have a favorite way to enjoy your vanilla ice cream?  üç¶ \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 22, 'total_tokens': 80, 'completion_time': 0.105454545, 'prompt_time': 0.002135055, 'queue_time': 0.016411472, 'total_time': 0.1075896}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d3d301d1-2893-46a8-af20-ed539693056a-0', usage_metadata={'input_tokens': 22, 'output_tokens': 58, 'total_tokens': 80})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":\"Jhon\"}}\n",
    "\n",
    "response3=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Jhon Cena and I like Vannila Ice cream\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Jhon Cena!  üí™ \\n\\nIs there something else you'd like to talk about? üé§  Perhaps your favorite wrestling move?  üòâ \\n\\n\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 91, 'total_tokens': 132, 'completion_time': 0.074545455, 'prompt_time': 0.005307046, 'queue_time': 0.018180212, 'total_time': 0.079852501}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-70ff7ee3-da07-4e47-bbe7-496461aba449-0', usage_metadata={'input_tokens': 91, 'output_tokens': 41, 'total_tokens': 132})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":\"Jhon\"}}\n",
    "\n",
    "response4=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You said you liked vanilla ice cream, Jhon Cena!  üç¶ \\n\\nDo you have a favorite topping for it?  Perhaps some chocolate sauce or sprinkles?  üòú \\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 144, 'total_tokens': 183, 'completion_time': 0.070909091, 'prompt_time': 0.005972765, 'queue_time': 0.022402395, 'total_time': 0.076881856}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7920745-15cf-465f-a491-d7b61c19ecbc-0', usage_metadata={'input_tokens': 144, 'output_tokens': 39, 'total_tokens': 183})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":\"Jhon\"}}\n",
    "\n",
    "response5=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is favourite item\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt1=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_chain=prompt1|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bala!\\n\\nIt's nice to meet you.  What can I do for you today?  \\n\\nI'm ready for your questions! üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 30, 'total_tokens': 67, 'completion_time': 0.067272727, 'prompt_time': 0.002117765, 'queue_time': 0.018852483, 'total_time': 0.069390492}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee770267-42a6-4b74-bed3-175e876fdf92-0', usage_metadata={'input_tokens': 30, 'output_tokens': 37, 'total_tokens': 67})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Bala\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(new_chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bala, it's nice to meet you! üëã \\n\\nI'm ready to answer your questions. What can I do for you today? üòä  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 30, 'total_tokens': 66, 'completion_time': 0.065454545, 'prompt_time': 0.002131295, 'queue_time': 0.018797714, 'total_time': 0.06758584}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-8feb3633-af4e-4e11-ad75-d47f324ca92a-0', usage_metadata={'input_tokens': 30, 'output_tokens': 36, 'total_tokens': 66})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "\n",
    "response5=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Bala\")],\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translator Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç Bala! üòä  ‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øà‡Æ™‡Øç ‡Æ™‡Æ£‡Æø‡Æµ‡Æø‡Æü‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ  ‡Æ§‡ÆØ‡Ææ‡Æ∞‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. ‡Æé‡Æ©‡Øç‡Æ©‡Ææ‡Æ≤‡Øç ‡Æâ‡Æ§‡Æµ‡Æø ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Ææ? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 31, 'total_tokens': 78, 'completion_time': 0.085454545, 'prompt_time': 0.002357194, 'queue_time': 0.018929364, 'total_time': 0.087811739}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f301eb8-9f31-4a3a-b762-ce1576936698-0', usage_metadata={'input_tokens': 31, 'output_tokens': 47, 'total_tokens': 78})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt2=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer the questions to the best of your ability in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_chain1=prompt2|model\n",
    "new_chain1.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Bala\")],\"language\":\"Tamil\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming - use when we need to trim the long history of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi! I'm Bala\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like Vannila Ice Cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2+2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No Problem', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having Fun', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=60,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi! I'm Bala\"),\n",
    "    AIMessage(content=\"Hi!\"),\n",
    "    HumanMessage(content=\"I like Vannila Ice Cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2+2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"No Problem\"),\n",
    "    HumanMessage(content=\"Having Fun\"),\n",
    "    AIMessage(content=\"Yes!\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step followed in below cell\n",
    "- Getting chat history from messages\n",
    "- importing messages in itemgetter\n",
    "- Using trimmer to trim the chat history\n",
    "- creating chains for messgaes | trimmer | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  messages: RunnableLambda(itemgetter('messages'))\n",
       "            | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000270BEF045E0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer the questions to the best of your ability in {language}'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000270E000A450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000270E001BDA0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "trim_chain=(RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) | prompt2 | model )\n",
    "trim_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You said you like vanilla ice cream!  üç¶ üòä  \\n\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 99, 'total_tokens': 114, 'completion_time': 0.027272727, 'prompt_time': 0.00430933, 'queue_time': 0.019292068, 'total_time': 0.031582057}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-27768b77-5186-43c3-bdb3-a46e66f29844-0', usage_metadata={'input_tokens': 99, 'output_tokens': 15, 'total_tokens': 114})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=trim_chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
